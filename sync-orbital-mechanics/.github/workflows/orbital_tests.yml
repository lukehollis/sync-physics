name: Orbital Mechanics Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'api/simulations/**'
      - 'tests/**'
      - '.github/workflows/orbital_tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'api/simulations/**'
      - 'tests/**'
  schedule:
    # Run daily validation at 00:00 UTC
    - cron: '0 0 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  test-physics:
    name: Physics Accuracy Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy matplotlib
          pip install pytest pytest-cov pytest-benchmark
          pip install psutil
      
      - name: Test Planetary Positions
        run: |
          python -m pytest tests/test_planetary_positions.py -v --tb=short
        continue-on-error: false
      
      - name: Test Lambert Solver and Trajectories
        run: |
          python -m pytest tests/test_trajectories.py -v --tb=short
        continue-on-error: false
      
      - name: Test N-Body Integration
        run: |
          python -m pytest tests/test_nbody_integration.py -v --tb=short
        continue-on-error: false
      
      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: physics-test-reports
          path: tests/reports/
          retention-days: 30

  test-performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy matplotlib
          pip install pytest pytest-cov pytest-benchmark
          pip install psutil
      
      - name: Run Performance Tests
        run: |
          python -m pytest tests/test_performance.py -v --tb=short --benchmark-only
        continue-on-error: true  # Performance tests are informational
      
      - name: Generate Performance Report
        if: always()
        run: |
          python tests/test_performance.py 2>&1 | tee performance_report.txt
      
      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-reports
          path: |
            tests/reports/performance_*.json
            performance_report.txt
          retention-days: 30
      
      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance_report.txt', 'utf8');
            const summary = report.split('\n').slice(0, 20).join('\n');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Performance Test Results\n\`\`\`\n${summary}\n\`\`\`\n[Full report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`
            });

  daily-validation:
    name: Daily JPL Horizons Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy
          # In production, would also install: pip install astroquery
      
      - name: Run Daily Validation
        run: |
          python tests/daily_validation.py
      
      - name: Upload validation report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: validation-reports
          path: tests/validation_reports/
          retention-days: 90
      
      - name: Create issue if validation fails
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const date = new Date().toISOString().split('T')[0];
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üö® Validation Failure - ${date}`,
              body: `Daily validation against JPL Horizons failed on ${date}.\n\nPlease check the [workflow run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`,
              labels: ['bug', 'validation', 'high-priority']
            });

  integration-tests:
    name: Full Integration Tests
    runs-on: ubuntu-latest
    needs: [test-physics]  # Only run if physics tests pass
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy matplotlib
          pip install pytest pytest-cov
          pip install psutil
      
      - name: Run All Tests
        run: |
          python tests/run_all_tests.py
      
      - name: Generate Coverage Report
        if: always()
        run: |
          pytest tests/ --cov=api.simulations --cov-report=xml --cov-report=html
        continue-on-error: true
      
      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: orbit-engine-coverage
        continue-on-error: true
      
      - name: Upload test summary
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-summary
          path: tests/reports/summary_*.json
          retention-days: 30

  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [test-physics, test-performance, integration-tests]
    if: always()
    
    steps:
      - name: Check test status
        run: |
          echo "Physics tests: ${{ needs.test-physics.result }}"
          echo "Performance tests: ${{ needs.test-performance.result }}"
          echo "Integration tests: ${{ needs.integration-tests.result }}"
      
      - name: Set status badge
        if: github.ref == 'refs/heads/main'
        run: |
          # This would update a status badge in README or external service
          echo "Status: ${{ needs.test-physics.result == 'success' && needs.integration-tests.result == 'success' && '‚úÖ Passing' || '‚ùå Failing' }}"
